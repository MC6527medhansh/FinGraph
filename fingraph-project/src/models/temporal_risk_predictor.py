"""
FinGraph Temporal Risk Predictor - FINAL FIX
Replace your existing temporal_risk_predictor.py with this version

This version properly handles MultiIndex columns from Yahoo Finance
"""

import sys
import os
import json
from pathlib import Path

sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import logging
import warnings
warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TemporalFeatureExtractor:
    """
    Extracts point-in-time features that exist at time T
    These features change daily and can predict future risk
    """
    
    def __init__(self):
        self.feature_names = [
            'returns_1d', 'returns_5d', 'returns_20d',  # Price momentum
            'volatility_20d', 'volatility_60d',          # Risk measures
            'rsi', 'price_vs_sma20', 'price_vs_sma50',   # Technical indicators
            'volume_ratio',                               # Volume activity
            'vix_proxy', 'rate_proxy', 'sector_momentum'  # Market context
        ]
    
    def extract_daily_features(self, stock_data, date, symbol):
        """
        Extract features that exist at date T for symbol
        Fixed to handle MultiIndex columns properly
        """
        try:
            # Ensure date is pandas Timestamp
            if isinstance(date, str):
                date = pd.to_datetime(date)
            
            # Debug: Check data availability
            symbol_data = stock_data[stock_data['Symbol'] == symbol].copy()
            if len(symbol_data) == 0:
                logger.debug(f"No data found for symbol {symbol}")
                return None
            
            # Get historical data up to (but not including) date T
            historical = symbol_data[symbol_data.index < date].copy()
            
            if len(historical) < 20:  # Reduced requirement for small datasets
                logger.debug(f"Insufficient history for {symbol} on {date}: {len(historical)} records")
                return None
            
            # Debug: Check required columns and fix MultiIndex if needed
            historical = self._fix_multiindex_columns(historical)
            
            required_cols = ['Close', 'Volume', 'Open', 'High', 'Low']
            missing_cols = [col for col in required_cols if col not in historical.columns]
            if missing_cols:
                logger.debug(f"Missing required columns for {symbol}: {missing_cols}")
                logger.debug(f"Available columns: {list(historical.columns)}")
                return None
            
            # Check for valid data
            if historical['Close'].isna().all():
                logger.debug(f"All Close prices are NaN for {symbol}")
                return None
            
            features = []
            
            # 1. PRICE MOMENTUM (looking backward from T)
            returns = historical['Close'].pct_change().fillna(0)
            
            # Ensure we have valid returns
            if returns.isna().all():
                logger.debug(f"All returns are NaN for {symbol}")
                return None
            
            features.extend([
                float(returns.iloc[-1] if len(returns) > 0 and pd.notna(returns.iloc[-1]) else 0.0),  # 1d return
                float(returns.iloc[-5:].mean() if len(returns) >= 5 else 0.0),  # 5d avg return
                float(returns.iloc[-20:].mean() if len(returns) >= 20 else 0.0),  # 20d avg return
            ])
            
            # 2. VOLATILITY (risk measures at T)
            vol_20 = returns.iloc[-20:].std() * np.sqrt(252) if len(returns) >= 20 else 0.2
            vol_60 = returns.iloc[-60:].std() * np.sqrt(252) if len(returns) >= 60 else 0.2
            
            # Cap volatility to reasonable range
            vol_20 = float(min(max(vol_20, 0.05), 2.0) if not np.isnan(vol_20) else 0.2)
            vol_60 = float(min(max(vol_60, 0.05), 2.0) if not np.isnan(vol_60) else 0.2)
            
            features.extend([vol_20, vol_60])
            
            # 3. TECHNICAL INDICATORS (state at T)
            close_prices = historical['Close'].fillna(method='ffill')
            
            # RSI
            rsi = self._calculate_rsi(close_prices, window=14)
            rsi_value = float(rsi.iloc[-1] if len(rsi) > 0 and pd.notna(rsi.iloc[-1]) else 50.0)
            features.append(rsi_value / 100.0)  # Normalize to 0-1
            
            # Moving averages
            sma_20 = float(close_prices.iloc[-20:].mean() if len(close_prices) >= 20 else close_prices.iloc[-1])
            sma_50 = float(close_prices.iloc[-50:].mean() if len(close_prices) >= 50 else close_prices.iloc[-1])
            current_price = float(close_prices.iloc[-1])
            
            price_vs_sma20 = (current_price / sma_20 - 1) if sma_20 > 0 else 0.0
            price_vs_sma50 = (current_price / sma_50 - 1) if sma_50 > 0 else 0.0
            
            # Cap the ratios to reasonable range
            price_vs_sma20 = float(max(-0.5, min(0.5, price_vs_sma20)))
            price_vs_sma50 = float(max(-0.5, min(0.5, price_vs_sma50)))
            
            features.extend([price_vs_sma20, price_vs_sma50])
            
            # 4. VOLUME INDICATORS
            volumes = historical['Volume'].fillna(method='ffill')
            if volumes.isna().all() or (volumes == 0).all():
                logger.debug(f"Invalid volume data for {symbol}")
                volume_ratio = 1.0
            else:
                avg_volume = float(volumes.iloc[-20:].mean() if len(volumes) >= 20 else volumes.iloc[-1])
                current_volume = float(volumes.iloc[-1])
                volume_ratio = current_volume / avg_volume if avg_volume > 0 else 1.0
                volume_ratio = max(0.1, min(10.0, volume_ratio))  # Cap ratio
            
            features.append(float(np.log(volume_ratio + 1)))
            
            # 5. MARKET CONTEXT (simplified proxies)
            recent_volatility = returns.iloc[-5:].std() if len(returns) >= 5 else 0.02
            features.extend([
                float(min(recent_volatility * 100, 1.0)),  # VIX proxy
                0.05,  # Interest rate proxy
                float(returns.iloc[-5:].mean() if len(returns) >= 5 else 0.0),  # Momentum proxy
            ])
            
            # Validate features
            features = np.array(features, dtype=np.float32)
            
            # Check for invalid values
            if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                logger.debug(f"Invalid feature values for {symbol} on {date}: {features}")
                # Replace invalid values
                features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # Final validation - ensure we have the expected number of features
            if len(features) != len(self.feature_names):
                logger.debug(f"Feature count mismatch for {symbol}: expected {len(self.feature_names)}, got {len(features)}")
                return None
            
            logger.debug(f"Successfully extracted features for {symbol} on {date}: {features}")
            return features
            
        except Exception as e:
            logger.debug(f"Failed to extract features for {symbol} on {date}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    def _fix_multiindex_columns(self, df):
        """Fix MultiIndex columns from Yahoo Finance"""
        try:
            if isinstance(df.columns, pd.MultiIndex):
                # Flatten multi-level columns by taking first level
                df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
                logger.debug(f"Fixed MultiIndex columns: {list(df.columns)}")
            return df
        except Exception as e:
            logger.debug(f"Error fixing MultiIndex columns: {e}")
            return df
    
    def _calculate_rsi(self, prices, window=14):
        """Calculate RSI technical indicator"""
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi.fillna(50)  # Fill NaN with neutral RSI
        except:
            return pd.Series([50.0] * len(prices), index=prices.index)

class RiskLabelCalculator:
    """
    Calculates forward-looking risk labels
    Risk measured from T+1 to T+horizon
    """
    
    def calculate_forward_risk(self, stock_data, start_date, symbol, horizon_days=21):
        """
        Calculate risk from start_date to start_date + horizon_days
        Fixed to handle MultiIndex columns properly
        """
        try:
            if isinstance(start_date, str):
                start_date = pd.to_datetime(start_date)
            
            end_date = start_date + timedelta(days=horizon_days + 5)  # Add buffer for weekends
            
            # Get future data from T+1 to T+horizon  
            future_data = stock_data[
                (stock_data['Symbol'] == symbol) & 
                (stock_data.index > start_date) &
                (stock_data.index <= end_date)
            ].copy()
            
            if len(future_data) < 3:  # Reduced requirement
                logger.debug(f"Insufficient future data for {symbol} from {start_date}: {len(future_data)} records")
                return None
            
            # Fix MultiIndex columns if needed
            future_data = self._fix_multiindex_columns(future_data)
            
            # Check if Close prices exist and are valid
            if 'Close' not in future_data.columns:
                logger.debug(f"No Close column for {symbol}")
                return None
            
            if future_data['Close'].isna().all():
                logger.debug(f"All future Close prices are NaN for {symbol}")
                return None
            
            # Calculate future returns
            future_returns = future_data['Close'].pct_change().dropna()
            
            if len(future_returns) == 0:
                logger.debug(f"No valid future returns for {symbol} from {start_date}")
                return None
            
            # Check for valid returns (not all zero or NaN)
            if future_returns.isna().all() or (future_returns == 0).all():
                logger.debug(f"Invalid future returns for {symbol}: all NaN or zero")
                return None
            
            # 1. Volatility Risk (annualized)
            volatility = float(future_returns.std() * np.sqrt(252))
            volatility = min(max(volatility, 0.05), 2.0) if not np.isnan(volatility) else 0.3
            
            # 2. Value at Risk (10th percentile for small samples)
            var_90 = float(np.percentile(future_returns, 10))  # Less extreme than 5th percentile
            if np.isnan(var_90):
                var_90 = -0.02  # Default 2% loss
            
            # 3. Maximum Drawdown
            if len(future_data) > 1:
                prices = future_data['Close'].fillna(method='ffill')
                cumulative = (1 + future_returns).cumprod()
                rolling_max = cumulative.expanding().max()
                drawdowns = (cumulative - rolling_max) / rolling_max
                max_drawdown = float(drawdowns.min())
                
                if np.isnan(max_drawdown):
                    max_drawdown = float(future_returns.min() if len(future_returns) > 0 else 0)
            else:
                max_drawdown = float(future_returns.iloc[0] if len(future_returns) > 0 else 0)
            
            # 4. Extreme Loss Days
            extreme_loss_pct = float((future_returns < -0.03).mean())  # 3% threshold
            if np.isnan(extreme_loss_pct):
                extreme_loss_pct = 0.0
            
            # 5. Composite Risk Score (0-1 scale)
            vol_score = min(volatility / 0.6, 1.0)
            var_score = min(abs(var_90) / 0.06, 1.0)  # 6% daily loss threshold
            drawdown_score = min(abs(max_drawdown) / 0.2, 1.0)
            extreme_score = extreme_loss_pct
            
            # Weighted composite
            composite_risk = float(
                0.35 * vol_score + 
                0.25 * var_score + 
                0.25 * drawdown_score + 
                0.15 * extreme_score
            )
            
            # Ensure composite is in valid range
            composite_risk = max(0.0, min(1.0, composite_risk))
            
            # Validate all outputs
            risk_data = {
                'volatility': volatility,
                'var_90': var_90,
                'max_drawdown': max_drawdown,
                'extreme_loss_pct': extreme_loss_pct,
                'composite_risk': composite_risk,
                'high_risk_binary': 1 if composite_risk > 0.6 else 0  # Adjusted threshold
            }
            
            # Check for any NaN values in the result
            for key, value in risk_data.items():
                if isinstance(value, (int, float)) and np.isnan(value):
                    logger.debug(f"NaN value in risk calculation for {symbol}: {key} = {value}")
                    return None
            
            logger.debug(f"Successfully calculated risk for {symbol} from {start_date}: {risk_data}")
            return risk_data
            
        except Exception as e:
            logger.debug(f"Failed to calculate risk for {symbol} from {start_date}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return None
    
    def _fix_multiindex_columns(self, df):
        """Fix MultiIndex columns from Yahoo Finance"""
        try:
            if isinstance(df.columns, pd.MultiIndex):
                # Flatten multi-level columns by taking first level
                df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
                logger.debug(f"Fixed MultiIndex columns: {list(df.columns)}")
            return df
        except Exception as e:
            logger.debug(f"Error fixing MultiIndex columns: {e}")
            return df

class TemporalDatasetBuilder:
    """
    Builds temporal dataset with proper time alignment
    """
    
    def __init__(self):
        self.feature_extractor = TemporalFeatureExtractor()
        self.risk_calculator = RiskLabelCalculator()
    
    def build_temporal_dataset(self, stock_data, symbols=None, min_history_days=20, horizon_days=21):
        """
        Build complete temporal dataset with improved error handling and MultiIndex fix
        """
        logger.info("🔧 Building temporal dataset...")
        
        # Fix MultiIndex columns and date index if needed
        stock_data = self._fix_data_structure(stock_data)
        
        if symbols is None:
            symbols = stock_data['Symbol'].unique()
        
        temporal_samples = []
        total_attempts = 0
        successful_features = 0
        successful_risks = 0
        
        # Debug information about the dataset
        logger.info(f"📊 Dataset info:")
        logger.info(f"  Total rows: {len(stock_data)}")
        logger.info(f"  Columns: {list(stock_data.columns)}")
        logger.info(f"  Date range: {stock_data.index.min()} to {stock_data.index.max()}")
        logger.info(f"  Symbols: {symbols}")
        
        for symbol in symbols:
            logger.info(f"Processing {symbol}...")
            
            symbol_data = stock_data[stock_data['Symbol'] == symbol].copy()
            if len(symbol_data) < min_history_days + horizon_days:
                logger.warning(f"Insufficient data for {symbol}: {len(symbol_data)} records")
                continue
            
            # Get available dates and ensure they're properly sorted
            available_dates = sorted(symbol_data.index.unique())
            logger.debug(f"{symbol} has {len(available_dates)} dates from {available_dates[0]} to {available_dates[-1]}")
            
            # Sample dates to avoid processing every single date (for efficiency)
            step_size = max(1, len(available_dates) // 100)  # Sample ~100 dates per symbol
            sampled_dates = available_dates[min_history_days:-horizon_days:step_size]
            
            logger.debug(f"{symbol}: Processing {len(sampled_dates)} sampled dates")
            
            for date in sampled_dates:
                total_attempts += 1
                
                # Extract features at time T
                features = self.feature_extractor.extract_daily_features(stock_data, date, symbol)
                if features is None:
                    logger.debug(f"Feature extraction failed for {symbol} on {date}")
                    continue
                
                successful_features += 1
                
                # Calculate risk from T+1 to T+horizon
                risk_data = self.risk_calculator.calculate_forward_risk(
                    stock_data, date, symbol, horizon_days
                )
                if risk_data is None:
                    logger.debug(f"Risk calculation failed for {symbol} on {date}")
                    continue
                
                successful_risks += 1
                
                # Create sample
                sample = {
                    'date': date,
                    'symbol': symbol,
                    'features': features,
                    'risk_score': risk_data['composite_risk'],
                    'high_risk': risk_data['high_risk_binary'],
                    'volatility': risk_data['volatility'],
                    'var_90': risk_data['var_90'],
                    'max_drawdown': risk_data['max_drawdown']
                }
                
                temporal_samples.append(sample)
        
        logger.info(f"📊 Processing summary:")
        logger.info(f"  Total attempts: {total_attempts}")
        logger.info(f"  Successful features: {successful_features}")
        logger.info(f"  Successful risks: {successful_risks}")
        logger.info(f"  Final samples: {len(temporal_samples)}")
        
        if len(temporal_samples) == 0:
            logger.error("❌ No temporal samples created!")
            logger.error("🔍 Final debug info:")
            logger.error(f"  Stock data shape: {stock_data.shape}")
            logger.error(f"  Date range: {stock_data.index.min()} to {stock_data.index.max()}")
            logger.error(f"  Symbols: {stock_data['Symbol'].unique()}")
            logger.error(f"  Columns: {list(stock_data.columns)}")
            return pd.DataFrame()
        
        df = pd.DataFrame(temporal_samples)
        logger.info(f"✅ Created {len(df)} temporal samples")
        logger.info(f"📅 Date range: {df['date'].min()} to {df['date'].max()}")
        logger.info(f"🎯 High risk ratio: {df['high_risk'].mean():.2%}")
        
        return df
    
    def _fix_data_structure(self, stock_data):
        """Fix both MultiIndex columns and corrupted date indexes"""
        try:
            # Fix MultiIndex columns
            if isinstance(stock_data.columns, pd.MultiIndex):
                logger.info("🔧 Fixing MultiIndex columns...")
                # Flatten multi-level columns by taking first level
                stock_data.columns = [col[0] if isinstance(col, tuple) else col for col in stock_data.columns]
                logger.info(f"✅ Fixed columns: {list(stock_data.columns)}")
            
            # Fix corrupted dates (1970 epoch issue)
            stock_data = self._fix_date_index(stock_data)
            
            return stock_data
            
        except Exception as e:
            logger.warning(f"Failed to fix data structure: {e}")
            return stock_data
    
    def _fix_date_index(self, stock_data):
        """Fix corrupted date indexes"""
        try:
            # Check if dates are corrupted (1970 epoch issue)
            min_date = stock_data.index.min()
            if pd.to_datetime('1970-01-01') <= min_date <= pd.to_datetime('1970-12-31'):
                logger.warning("🔧 Detected corrupted dates (1970 epoch), attempting to fix...")
                
                # Try to reconstruct dates based on row count and symbols
                symbols = stock_data['Symbol'].unique()
                n_symbols = len(symbols)
                total_rows = len(stock_data)
                rows_per_symbol = total_rows // n_symbols
                
                # Create new date range (assume recent 4 years of daily data)
                end_date = datetime.now().date()
                start_date = end_date - timedelta(days=rows_per_symbol)
                new_dates = pd.date_range(start=start_date, end=end_date, freq='D')[:rows_per_symbol]
                
                # Assign dates to each symbol
                new_index = []
                for symbol in symbols:
                    symbol_data = stock_data[stock_data['Symbol'] == symbol]
                    symbol_dates = new_dates[:len(symbol_data)]
                    new_index.extend(symbol_dates)
                
                # Update the index
                stock_data.index = pd.DatetimeIndex(new_index[:len(stock_data)])
                logger.info(f"✅ Fixed dates: new range {stock_data.index.min()} to {stock_data.index.max()}")
            
            return stock_data
            
        except Exception as e:
            logger.warning(f"Failed to fix date index: {e}")
            return stock_data

# Include all the other classes (TemporalTrainTestSplit, BaselineModels, SimpleGNNModel, TemporalRiskPredictor)
# exactly as they were in the previous version, but with the updated load_stock_data method

class TemporalTrainTestSplit:
    """Creates proper temporal splits with no data leakage"""
    
    def create_temporal_splits(self, temporal_df, train_ratio=0.6, val_ratio=0.2):
        """Split data maintaining temporal order"""
        logger.info("📅 Creating temporal splits...")
        
        df_sorted = temporal_df.sort_values('date').copy()
        unique_dates = sorted(df_sorted['date'].unique())
        n_dates = len(unique_dates)
        
        train_end_idx = int(n_dates * train_ratio)
        val_end_idx = int(n_dates * (train_ratio + val_ratio))
        
        train_end_date = unique_dates[train_end_idx - 1]
        val_end_date = unique_dates[val_end_idx - 1]
        
        train_df = df_sorted[df_sorted['date'] <= train_end_date].copy()
        val_df = df_sorted[
            (df_sorted['date'] > train_end_date) & 
            (df_sorted['date'] <= val_end_date)
        ].copy()
        test_df = df_sorted[df_sorted['date'] > val_end_date].copy()
        
        # Validate no leakage
        if len(train_df) > 0 and len(val_df) > 0:
            assert train_df['date'].max() < val_df['date'].min()
        if len(val_df) > 0 and len(test_df) > 0:
            assert val_df['date'].max() < test_df['date'].min()
        
        logger.info(f"📊 Train: {len(train_df)} samples (until {train_end_date.date()})")
        logger.info(f"📊 Val:   {len(val_df)} samples (until {val_end_date.date()})")
        logger.info(f"📊 Test:  {len(test_df)} samples (from {test_df['date'].min().date() if len(test_df) > 0 else 'N/A'})")
        logger.info("✅ No temporal leakage confirmed")
        
        return {
            'train': train_df,
            'val': val_df,
            'test': test_df,
            'split_dates': {
                'train_end': train_end_date,
                'val_end': val_end_date,
                'test_start': test_df['date'].min() if len(test_df) > 0 else None
            }
        }

class BaselineModels:
    """Proper temporal baseline models for comparison"""
    
    def __init__(self):
        self.models = {}
    
    def train_logistic_regression(self, X_train, y_train):
        """Train logistic regression baseline"""
        model = LogisticRegression(random_state=42, max_iter=1000)
        y_binary = (y_train > 0.5).astype(int)
        model.fit(X_train, y_binary)
        self.models['logistic'] = model
        return model
    
    def train_random_forest(self, X_train, y_train):
        """Train random forest baseline"""
        model = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=10)
        model.fit(X_train, y_train)
        self.models['random_forest'] = model
        return model
    
    def predict_logistic(self, X_test):
        """Predict with logistic regression"""
        if 'logistic' not in self.models:
            raise ValueError("Logistic model not trained")
        return self.models['logistic'].predict_proba(X_test)[:, 1]
    
    def predict_random_forest(self, X_test):
        """Predict with random forest"""
        if 'random_forest' not in self.models:
            raise ValueError("Random forest model not trained")
        return self.models['random_forest'].predict(X_test)

class SimpleGNNModel(nn.Module):
    """Simplified GNN for temporal risk prediction"""
    
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim//2)
        self.fc3 = nn.Linear(hidden_dim//2, 1)
        self.dropout = nn.Dropout(0.3)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.sigmoid(self.fc3(x))
        return x.squeeze()

class TemporalRiskPredictor:
    """
    Complete temporal risk prediction system with MultiIndex column fix
    """
    
    def __init__(self):
        self.dataset_builder = TemporalDatasetBuilder()
        self.splitter = TemporalTrainTestSplit()
        self.baseline_models = BaselineModels()
        self.gnn_model = None
        
        # Storage
        self.stock_data = None
        self.temporal_df = None
        self.splits = None
        self.results = {}
    
    def load_stock_data(self, symbols=['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'], 
                       start_date='2022-01-01', end_date='2024-09-01'):
        """Load stock data with proper MultiIndex column handling"""
        logger.info(f"📊 Loading stock data for {len(symbols)} symbols...")
        
        all_data = []
        successful_symbols = []
        
        for symbol in symbols:
            try:
                logger.info(f"Downloading {symbol}...")
                data = yf.download(symbol, start=start_date, end=end_date, progress=False)
                
                if len(data) > 100:  # Ensure reasonable amount of data
                    # CRITICAL FIX: Handle MultiIndex columns properly
                    if isinstance(data.columns, pd.MultiIndex):
                        # Flatten multi-level columns by taking first level
                        data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]
                        logger.debug(f"Fixed MultiIndex columns for {symbol}: {list(data.columns)}")
                    
                    # Ensure required columns exist
                    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
                    missing_columns = [col for col in required_columns if col not in data.columns]
                    
                    if missing_columns:
                        logger.warning(f"⚠️ {symbol}: Missing columns {missing_columns}")
                        continue
                    
                    # Add symbol column
                    data['Symbol'] = symbol
                    
                    # Reset index to ensure date is a column, then set it back
                    data = data.reset_index()
                    if 'Date' in data.columns:
                        data.set_index('Date', inplace=True)
                    
                    # Ensure index is datetime
                    if not isinstance(data.index, pd.DatetimeIndex):
                        data.index = pd.to_datetime(data.index)
                    
                    # Sort by date
                    data = data.sort_index()
                    
                    # Remove any duplicate dates
                    data = data[~data.index.duplicated(keep='first')]
                    
                    # Validate data quality
                    if data['Close'].isna().sum() > len(data) * 0.1:  # More than 10% missing
                        logger.warning(f"⚠️ {symbol}: Too many missing close prices")
                        continue
                    
                    # Forward fill missing values
                    data[required_columns] = data[required_columns].fillna(method='ffill')
                    
                    all_data.append(data)
                    successful_symbols.append(symbol)
                    logger.info(f"✅ {symbol}: {len(data)} days")
                    logger.debug(f"   Columns: {list(data.columns)}")
                    logger.debug(f"   Date range: {data.index.min()} to {data.index.max()}")
                    
                else:
                    logger.warning(f"⚠️ {symbol}: Insufficient data ({len(data)} days)")
            except Exception as e:
                logger.warning(f"❌ Failed to load {symbol}: {e}")
                import traceback
                logger.debug(traceback.format_exc())
        
        if all_data:
            self.stock_data = pd.concat(all_data, ignore_index=False)
            
            # Final validation and cleaning
            self.stock_data = self._clean_stock_data(self.stock_data)
            
            logger.info(f"📈 Successfully loaded {len(successful_symbols)} symbols")
            logger.info(f"📈 Total records: {len(self.stock_data)}")
            logger.info(f"📅 Date range: {self.stock_data.index.min()} to {self.stock_data.index.max()}")
            logger.info(f"📊 Final columns: {list(self.stock_data.columns)}")
            
            return True
        else:
            logger.error("❌ No stock data loaded")
            return False
    
    def _clean_stock_data(self, data):
        """Clean and validate stock data"""
        logger.debug("🧹 Cleaning stock data...")
        
        # Remove any rows with all NaN values
        data = data.dropna(how='all')
        
        # Ensure numeric columns are numeric
        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in numeric_columns:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
        
        # Remove rows where Close price is 0 or negative
        if 'Close' in data.columns:
            data = data[data['Close'] > 0]
        
        # Remove rows where Volume is negative
        if 'Volume' in data.columns:
            data = data[data['Volume'] >= 0]
        
        # Sort by symbol and date
        if 'Symbol' in data.columns:
            data = data.sort_values(['Symbol', data.index.name or 'Date'])
        
        logger.debug(f"✅ Cleaned data: {len(data)} records remaining")
        return data
    
    def create_temporal_dataset(self):
        """Create temporal dataset with features and risk labels"""
        if self.stock_data is None:
            logger.error("❌ No stock data available. Load data first.")
            return False
        
        self.temporal_df = self.dataset_builder.build_temporal_dataset(self.stock_data)
        return len(self.temporal_df) > 0
    
    def create_train_test_splits(self):
        """Create temporal train/val/test splits"""
        if self.temporal_df is None or len(self.temporal_df) == 0:
            logger.error("❌ No temporal dataset. Create dataset first.")
            return False
        
        self.splits = self.splitter.create_temporal_splits(self.temporal_df)
        return True
    
    def train_models(self):
        """Train all models with better error handling"""
        if self.splits is None:
            logger.error("❌ No train/test splits. Create splits first.")
            return False
        
        logger.info("🤖 Training models...")
        
        try:
            # Prepare data
            X_train = np.vstack(self.splits['train']['features'].values)
            y_train = self.splits['train']['risk_score'].values
            X_val = np.vstack(self.splits['val']['features'].values)
            y_val = self.splits['val']['risk_score'].values
            X_test = np.vstack(self.splits['test']['features'].values)
            y_test = self.splits['test']['risk_score'].values
            
            logger.info(f"📊 Training data shapes: X_train={X_train.shape}, y_train={y_train.shape}")
            
            # Train baseline models
            logger.info("📊 Training baseline models...")
            self.baseline_models.train_logistic_regression(X_train, y_train)
            self.baseline_models.train_random_forest(X_train, y_train)
            
            # Train simple GNN
            logger.info("🧠 Training GNN model...")
            self.gnn_model = SimpleGNNModel(X_train.shape[1])
            optimizer = torch.optim.Adam(self.gnn_model.parameters(), lr=0.001)
            criterion = nn.MSELoss()
            
            # Training loop with early stopping
            X_train_tensor = torch.FloatTensor(X_train)
            y_train_tensor = torch.FloatTensor(y_train)
            X_val_tensor = torch.FloatTensor(X_val)
            y_val_tensor = torch.FloatTensor(y_val)
            
            best_val_loss = float('inf')
            patience = 10
            patience_counter = 0
            
            for epoch in range(100):
                # Training
                self.gnn_model.train()
                optimizer.zero_grad()
                outputs = self.gnn_model(X_train_tensor)
                train_loss = criterion(outputs, y_train_tensor)
                train_loss.backward()
                optimizer.step()
                
                # Validation
                self.gnn_model.eval()
                with torch.no_grad():
                    val_outputs = self.gnn_model(X_val_tensor)
                    val_loss = criterion(val_outputs, y_val_tensor)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logger.info(f"Early stopping at epoch {epoch}")
                        break
                
                if epoch % 20 == 0:
                    logger.info(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")
            
            logger.info("✅ All models trained")
            return True
            
        except Exception as e:
            logger.error(f"❌ Training failed: {e}")
            return False
    
    def evaluate_models(self):
        """Evaluate all models and compare performance"""
        if self.gnn_model is None:
            logger.error("❌ Models not trained. Train models first.")
            return False
        
        logger.info("📊 Evaluating models...")
        
        try:
            # Prepare test data
            X_test = np.vstack(self.splits['test']['features'].values)
            y_test = self.splits['test']['risk_score'].values
            
            # Get predictions
            pred_logistic = self.baseline_models.predict_logistic(X_test)
            pred_rf = self.baseline_models.predict_random_forest(X_test)
            
            self.gnn_model.eval()
            with torch.no_grad():
                pred_gnn = self.gnn_model(torch.FloatTensor(X_test)).numpy()
            
            # Calculate metrics
            mse_logistic = mean_squared_error(y_test, pred_logistic)
            mse_rf = mean_squared_error(y_test, pred_rf)
            mse_gnn = mean_squared_error(y_test, pred_gnn)
            
            self.results = {
                'Logistic Regression': {'mse': mse_logistic, 'predictions': pred_logistic},
                'Random Forest': {'mse': mse_rf, 'predictions': pred_rf},
                'Simple GNN': {'mse': mse_gnn, 'predictions': pred_gnn},
                'actual': y_test
            }
            
            # Print results
            logger.info("📈 Model Performance (MSE):")
            for model_name, metrics in self.results.items():
                if isinstance(metrics, dict) and 'mse' in metrics:
                    logger.info(f"  {model_name}: {metrics['mse']:.4f}")

            # Find best model
            model_scores = {name: metrics['mse'] for name, metrics in self.results.items()
                           if isinstance(metrics, dict) and 'mse' in metrics}
            best_model = min(model_scores, key=model_scores.get)
            logger.info(f"🏆 Best model: {best_model}")

            # Persist evaluation metrics for downstream services
            performance_summary = {
                'generated_at': datetime.utcnow().isoformat(),
                'best_model': best_model,
                'metrics': {
                    'Logistic Regression': {
                        'mse': float(mse_logistic),
                        'rmse': float(np.sqrt(mse_logistic))
                    },
                    'Random Forest': {
                        'mse': float(mse_rf),
                        'rmse': float(np.sqrt(mse_rf))
                    },
                    'Simple GNN': {
                        'mse': float(mse_gnn),
                        'rmse': float(np.sqrt(mse_gnn))
                    }
                }
            }

            try:
                performance_dir = Path(__file__).resolve().parents[2] / 'models'
                performance_dir.mkdir(parents=True, exist_ok=True)
                performance_path = performance_dir / 'performance.json'
                with performance_path.open('w', encoding='utf-8') as f:
                    json.dump(performance_summary, f, indent=4)
                logger.info(f"💾 Saved performance metrics to {performance_path}")
            except Exception as save_error:
                logger.error(f"❌ Failed to save performance metrics: {save_error}")

            return True

        except Exception as e:
            logger.error(f"❌ Evaluation failed: {e}")
            return False
    
    def run_complete_pipeline(self):
        """Run the complete temporal risk prediction pipeline"""
        logger.info("🚀 Starting Temporal Risk Prediction Pipeline")
        logger.info("="*60)
        
        steps = [
            ("Loading stock data", self.load_stock_data),
            ("Creating temporal dataset", self.create_temporal_dataset),
            ("Creating train/test splits", self.create_train_test_splits),
            ("Training models", self.train_models),
            ("Evaluating models", self.evaluate_models)
        ]
        
        for step_name, step_func in steps:
            logger.info(f"\n📋 {step_name}...")
            if not step_func():
                logger.error(f"❌ Failed at: {step_name}")
                return False
        
        logger.info("\n🎉 Pipeline completed successfully!")
        logger.info("\n💡 Key Achievements:")
        logger.info("  ✅ Fixed temporal issues with point-in-time features")
        logger.info("  ✅ Fixed MultiIndex column issues from Yahoo Finance")
        logger.info("  ✅ 21-day forward risk prediction")
        logger.info("  ✅ Proper temporal train/val/test splits")
        logger.info("  ✅ Multiple models trained and compared")
        logger.info("  ✅ No data leakage confirmed")
        
        return True

# Example usage and testing
def test_temporal_predictor():
    """Test the temporal risk predictor"""
    predictor = TemporalRiskPredictor()
    
    # Run with smaller dataset for testing
    success = predictor.run_complete_pipeline()
    
    if success:
        print("\n🎯 SUCCESS! Temporal risk prediction working correctly")
        print("📊 Check results in predictor.results")
        return predictor
    else:
        print("❌ Pipeline failed - check logs")
        return None

if __name__ == "__main__":
    test_temporal_predictor()